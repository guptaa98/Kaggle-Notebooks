{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text generation pytorch implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrUsZ7nm+/X6dXhw54i2YK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guptaa98/Kaggle-Notebooks/blob/master/text_generation_pytorch_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ1-5K4mBOlU"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import string\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93iwIyEF8xNy"
      },
      "source": [
        "data = \"i am implementing lstm\"\n",
        "\n",
        "#Length of the sequence\n",
        "seq_len = len(data)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ozYmxyoADA6",
        "outputId": "7f4da2c6-9975-43ca-db56-86496c5cf03a"
      },
      "source": [
        "seq_len"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6IcWzbQAa59",
        "outputId": "09c57943-6a69-4599-b555-1ef5272d3262"
      },
      "source": [
        "import string\n",
        "letters = string.ascii_lowercase+' #'\n",
        "n_letters = len(letters)\n",
        "print('Letter set is '+letters)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Letter set is abcdefghijklmnopqrstuvwxyz #\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0YwDycJAgZ8",
        "outputId": "a7210d9c-72de-451d-d6e1-d7ca9a634eca"
      },
      "source": [
        "#This function takes a character and returns an encoded vector. The encoding is one hot\n",
        "\n",
        "def ltt(ch):\n",
        "    ans = torch.zeros(n_letters)\n",
        "    ans[letters.find(ch)]=1\n",
        "    return ans\n",
        "    \n",
        "print(\"Encoding of 'a' \",ltt('a'))\n",
        "print(\"Encoding of 'b' \",ltt('b'))\n",
        "print(\"Encoding of '#' \",ltt('#'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding of 'a'  tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Encoding of 'b'  tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Encoding of '#'  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj_fvoebBKEL"
      },
      "source": [
        "#This is our neural network class. every Neural Network in pytorch extends nn.Module\n",
        "\n",
        "class MyLSTM(nn.Module):\n",
        "    def __init__(self , input_dim , hidden_dim):\n",
        "        super(MyLSTM,self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        #LSTM takes, input dimensions, hidden dimensions and num layers in case of stacked LSTMs (Default is 1)\n",
        "        self.LSTM = nn.LSTM(input_dim,hidden_dim)\n",
        "        \n",
        "    #Input must be 3 dimensional (Sequence len, batch, input dimensions)\n",
        "    #hc is a tuple which contains the vectors h (hidden/feedback) and c (cell state vector)\n",
        "\n",
        "    def forward(self,inp,hc):\n",
        "        #this gives output for each input and also (hidden and cell state vector)\n",
        "\n",
        "        output , _ = self.LSTM(inp,hc)\n",
        "        return output"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqfOyRYqBV-x"
      },
      "source": [
        "#Dimensions of output of neural network is (seq_len, batch , hidden_dim). Since we want output dimensions to be\n",
        "#the same as n_letters, hidden_dim = n_letters (output dimensions = hidden_dimensions)\n",
        "\n",
        "hidden_dim = n_letters     \n",
        "\n",
        "#Invoking model. Input dimensions = n_letters i.e 28. output dimensions = hidden_dimensions = 28\n",
        "\n",
        "model = MyLSTM(n_letters,hidden_dim)\n",
        "\n",
        "optimizer = torch.optim.Adam(params = model.parameters(),lr=0.01)\n",
        "\n",
        "LOSS = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N5dRY55hDTjD",
        "outputId": "8b632778-0430-4351-924e-90e0c1bcc4ce"
      },
      "source": [
        "letters"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'abcdefghijklmnopqrstuvwxyz #'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnqqU7LVBWBu",
        "outputId": "adc34605-aa8f-4617-c7c7-c0828010d3d4"
      },
      "source": [
        "#List to store targets\n",
        "targets = []\n",
        "\n",
        "#Iterate through all chars in the sequence, starting from second letter. Since output for 1st letter is the 2nd letter\n",
        "\n",
        "for x in data[1:]+'#':\n",
        "    #Find the target index. For a, it is 0, For 'b' it is 1 etc..\n",
        "    #print(letters.find(x))\n",
        "    targets.append(letters.find(x))\n",
        "#Convert into tensor\n",
        "#print(targets)\n",
        "targets = torch.tensor(targets)\n",
        "print(\"tensor targets\",targets)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor targets tensor([26,  0, 12, 26,  8, 12, 15, 11,  4, 12,  4, 13, 19,  8, 13,  6, 26, 11,\n",
            "        18, 19, 12, 27])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zddl8I59BWEC"
      },
      "source": [
        "#List to store input\n",
        "inp_ohe = []\n",
        "#Iterate through all inputs in the sequence\n",
        "for c in data:\n",
        "    #Convert into one hot encoding\n",
        "    inp_ohe.append(ltt(c))\n",
        "\n",
        "#Convert list to tensor\n",
        "inp = torch.cat(inp_ohe,dim=0)\n",
        "#Reshape tensor into 3 dimensions (sequence length, batches = 1, dimensions = n_letters (28))\n",
        "inp = inp.view(seq_len,1,n_letters)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcJWJb7rZRMg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNi6Jl8-BWGL",
        "outputId": "a4a48d77-39ee-4b0e-9aec-8198965d8622"
      },
      "source": [
        "print(inp[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uMaJQgBBWIa",
        "outputId": "c008b739-da6f-4c6d-d027-c3442f194120"
      },
      "source": [
        "#Let's note down start time to track the training time\n",
        "#import time\n",
        "#start = time.time()\n",
        "\n",
        "#Number of iterations\n",
        "n_iters = 50\n",
        "\n",
        "for itr in range(n_iters):\n",
        "    #Zero the previous gradients\n",
        "    model.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #Initialize h and c vectors\n",
        "    h = torch.rand(hidden_dim).view(1,1,hidden_dim)\n",
        "    c = torch.rand(hidden_dim).view(1,1,hidden_dim)\n",
        "\n",
        "    #Find the output\n",
        "    output = model(inp,(h,c))\n",
        "\n",
        "    #Reshape the output to 2 dimensions. This is done, so that we can compare with target and get loss\n",
        "    #print(output.shape)\n",
        "    output = output.view(seq_len,n_letters)\n",
        "    \n",
        "    #Find loss\n",
        "    loss = LOSS(output,targets)\n",
        "\n",
        "    #Print loss for every 10th iteration\n",
        "    if itr%10==0:\n",
        "        print(itr , ' ' , (loss) )\n",
        "    \n",
        "    #Back propagate the loss\n",
        "    loss.backward()\n",
        "    #Perform weight updation\n",
        "    optimizer.step()\n",
        "    \n",
        "#print((time.time()-start))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0   tensor(2.2903, grad_fn=<NllLossBackward>)\n",
            "10   tensor(2.2251, grad_fn=<NllLossBackward>)\n",
            "20   tensor(2.1688, grad_fn=<NllLossBackward>)\n",
            "30   tensor(2.1190, grad_fn=<NllLossBackward>)\n",
            "40   tensor(2.0874, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8UlUC9RH39q"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z8BOV59BWLR"
      },
      "source": [
        "#This utility method predicts the next letter given the sequence   \n",
        "def predict(s):\n",
        "\n",
        "    #Get the vector for input\n",
        "    inp = s\n",
        "\n",
        "    #Initialize h and c vectors\n",
        "    h = torch.rand(1,1,hidden_dim)\n",
        "    c = torch.rand(1,1,hidden_dim)\n",
        "    \n",
        "    #Get the output\n",
        "    out = model(inp,(h,c))\n",
        "    \n",
        "    #Find the corresponding letter from the output\n",
        "    return letters[out[-1][0].topk(1)[1].detach().numpy().item()]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omzDlFE3BWNn"
      },
      "source": [
        "test = \"i am imple\""
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwE7Sbl5BWQO"
      },
      "source": [
        "#List to store input\n",
        "inp_tohe = []\n",
        "#Iterate through all inputs in the sequence\n",
        "for c in test:\n",
        "    #Convert into one hot encoding\n",
        "    inp_tohe.append(ltt(c))\n",
        "\n",
        "#Convert list to tensor\n",
        "s = torch.cat(inp_tohe,dim=0)\n",
        "#Reshape tensor into 3 dimensions (sequence length, batches = 1, dimensions = n_letters (28))\n",
        "s = s.view(len(test),1,n_letters)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DFVgn-ytBWSp",
        "outputId": "ff05d429-c48c-46a3-d9b7-12353c9c963c"
      },
      "source": [
        "predict(s)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'m'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGEnZq8HIqgA"
      },
      "source": [
        "test = \"i\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}